from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import requests
import csv
import time
from webdriver_manager.chrome import ChromeDriverManager

# URL trang danh s√°ch vi·ªác l√†m
base_url = "https://careerviet.vn/viec-lam/cntt-phan-cung-mang-cntt-phan-mem-c63,1-trang-{}-vi.html"

# File CSV ƒë·ªÉ l∆∞u k·∫øt qu·∫£
csv_filename = "jobs.csv"

# ·∫®n danh
options = webdriver.ChromeOptions()
options.add_argument("--incognito")  # Ch·∫ø ƒë·ªô ·∫©n danh
options.add_argument("--disable-geolocation")  # V√¥ hi·ªáu h√≥a geolocation
options.add_argument("--disable-application-cache")  # X√≥a cache
options.add_argument("--disk-cache-size=0")  # B·ªè s·ª≠ d·ª•ng cache

# Kh·ªüi t·∫°o tr√¨nh duy·ªát
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

# C√†i t·ªça ƒë·ªô gi·∫£ (n·∫øu c·∫ßn)
driver.execute_cdp_cmd("Emulation.setGeolocationOverride", {
    "latitude": 0,  # Gi√° tr·ªã vƒ© ƒë·ªô (fake)
    "longitude": 0,  # Gi√° tr·ªã kinh ƒë·ªô (fake)
    "accuracy": 100  # ƒê·ªô ch√≠nh x√°c
})

# C·∫•u h√¨nh Selenium (ƒë√£ c√≥ options)
driver = webdriver.Chrome(service=service, options=options)

# M·ªü file CSV ƒë·ªÉ ghi d·ªØ li·ªáu
with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    # Ghi ti√™u ƒë·ªÅ c·ªôt
    writer.writerow(
        ["Job Title", "Job ID", "Company", "Salary", "Location", "Industry", "Experience", "Expire Date",
         "Updated Date", "Position", "Job Requirement", "Job Link"])

    page = 20  # B·∫Øt ƒë·∫ßu t·ª´ trang 1
    max_pages = 24  # S·ªë trang t·ªëi ƒëa c·∫ßn c√†o
    while page <= max_pages:
        print(f"üìÑ ƒêang c√†o d·ªØ li·ªáu trang {page}...")
        url = base_url.format(page)

        # Selenium t·∫£i trang
        driver.get(url)
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, 'job-item')))

        # Cu·ªôn xu·ªëng ƒë·ªÉ t·∫£i th√™m c√¥ng vi·ªác
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # L·∫•y m√£ ngu·ªìn trang sau khi t·∫£i xong
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # T√¨m t·∫•t c·∫£ c√°c block ch·ª©a th√¥ng tin vi·ªác l√†m
        job_blocks = soup.find_all('div', class_='job-item')

        # In ra s·ªë l∆∞·ª£ng c√¥ng vi·ªác
        print(f"S·ªë l∆∞·ª£ng c√¥ng vi·ªác tr√™n trang {page}: {len(job_blocks)}")

        # N·∫øu kh√¥ng c√≥ c√¥ng vi·ªác n√†o -> D·ª´ng l·∫°i
        if not job_blocks:
            print("üöÄ Ho√†n th√†nh! Kh√¥ng c√≤n d·ªØ li·ªáu ƒë·ªÉ c√†o.")
            break

        # Duy·ªát qua t·ª´ng c√¥ng vi·ªác
        for job_block in job_blocks:
            # üÜî ID c√¥ng vi·ªác
            job_id = job_block.get('id', '').replace('job-item-', '')

            # üìå Ti√™u ƒë·ªÅ c√¥ng vi·ªác
            title_tag = job_block.find('div', class_='title')
            job_title = title_tag.text.strip() if title_tag else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"

            # üîó Link c√¥ng vi·ªác
            job_link = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else "Kh√¥ng c√≥ link"

            # üè¢ C√¥ng ty
            company_tag = job_block.find('a', class_='company-name')
            company = company_tag.text.strip() if company_tag else "Kh√¥ng c√≥ th√¥ng tin c√¥ng ty"

            # üí∞ L∆∞∆°ng
            salary_tag = job_block.find('div', class_='salary')
            salary = salary_tag.text.strip().replace("L∆∞∆°ng: ", "") if salary_tag else "Kh√¥ng c√≥ th√¥ng tin l∆∞∆°ng"

            # üìç ƒê·ªãa ƒëi·ªÉm
            location_tag = job_block.find('div', class_='location')
            location = location_tag.text.strip() if location_tag else "Kh√¥ng c√≥ th√¥ng tin ƒë·ªãa ƒëi·ªÉm"

            # üìÖ H·∫°n n·ªôp
            expire_tag = job_block.find('div', class_='expire-date')
            expire_date = expire_tag.text.strip().replace("H·∫°n n·ªôp: ", "") if expire_tag else "Kh√¥ng c√≥ h·∫°n n·ªôp"

            # üè≠ C√†o ng√†nh ngh·ªÅ, kinh nghi·ªám, ng√†y c·∫≠p nh·∫≠t, c·∫•p b·∫≠c & m√¥ t·∫£ c√¥ng vi·ªác t·ª´ trang chi ti·∫øt
            industry = "Kh√¥ng c√≥ th√¥ng tin ng√†nh ngh·ªÅ"
            experience = "Kh√¥ng c√≥ th√¥ng tin kinh nghi·ªám"
            updated_date = "Kh√¥ng c√≥ ng√†y c·∫≠p nh·∫≠t"
            position = "Kh√¥ng c√≥ th√¥ng tin c·∫•p b·∫≠c"
            job_requirement = "Kh√¥ng c√≥ y√™u c·∫ßu c√¥ng vi·ªác"

            try:
                print(f"üîó ƒêang truy c·∫≠p: {job_link}")  # Debug URL trang chi ti·∫øt
                job_response = requests.get(job_link, headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                })

                if job_response.status_code == 200:
                    job_soup = BeautifulSoup(job_response.content, 'html.parser')

                    # T√¨m t·∫•t c·∫£ c√°c strong v√† ki·ªÉm tra xem c√≥ ch·ª©a c√°c th√¥ng tin c·∫ßn thi·∫øt hay kh√¥ng
                    all_strong_tags = job_soup.find_all('strong')
                    for strong_tag in all_strong_tags:
                        # Ng√†nh ngh·ªÅ
                        if "Ng√†nh ngh·ªÅ" in strong_tag.text:
                            industry_paragraph = strong_tag.find_next('p')
                            if industry_paragraph:
                                industry_links = industry_paragraph.find_all('a')
                                industry = ", ".join(link.text.strip() for link in industry_links if link.text.strip())
                            break

                    for strong_tag in all_strong_tags:
                        # Kinh nghi·ªám
                        if "Kinh nghi·ªám" in strong_tag.text:
                            experience_paragraph = strong_tag.find_next('p')
                            if experience_paragraph:
                                experience = " ".join(
                                    " ".join(line.split()) for line in experience_paragraph.stripped_strings)
                            break

                    for strong_tag in all_strong_tags:
                        # Ng√†y c·∫≠p nh·∫≠t
                        if "Ng√†y c·∫≠p nh·∫≠t" in strong_tag.text:
                            updated_date_paragraph = strong_tag.find_next('p')
                            if updated_date_paragraph:
                                updated_date = updated_date_paragraph.text.strip()
                            break

                    for strong_tag in all_strong_tags:
                        # C·∫•p b·∫≠c
                        if "C·∫•p b·∫≠c" in strong_tag.text:
                            position_paragraph = strong_tag.find_next('p')
                            if position_paragraph:
                                position = position_paragraph.text.strip()
                            break

                    # Y√™u c·∫ßu c√¥ng vi·ªác (l·∫•y l·∫ßn xu·∫•t hi·ªán th·ª© hai c·ªßa class)
                    job_requirement_divs = job_soup.find_all('div', class_='detail-row reset-bullet')
                    if len(job_requirement_divs) > 1:
                        job_requirement = " ".join(
                            " ".join(line.split()) for line in job_requirement_divs[1].stripped_strings)

            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi l·∫•y th√¥ng tin chi ti·∫øt c·ªßa {job_title}: {e}")

            # X·ª≠ l√Ω ƒë·ªÉ ƒë·∫£m b·∫£o c√°c chu·ªói kh√¥ng g√¢y l·ªói CSV
            row = [
                job_title.replace("\n", " ").strip(),
                job_id,
                company.replace("\n", " ").strip(),
                salary.replace("\n", " ").strip(),
                location.replace("\n", " ").strip(),
                industry.replace("\n", " ").strip(),
                experience.replace("\n", " ").strip(),
                expire_date.replace("\n", " ").strip(),
                updated_date.replace("\n", " ").strip(),
                position.replace("\n", " ").strip(),
                job_requirement.replace("\n", " ").strip(),
                job_link.replace("\n", " ").strip()
            ]

            # Ghi d·ªØ li·ªáu v√†o file CSV
            writer.writerow(row)

        # Ch·ªù 5 gi√¢y ƒë·ªÉ tr√°nh b·ªã ch·∫∑n (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
        time.sleep(2)

        # TƒÉng s·ªë trang
        page += 1

# ƒê√≥ng tr√¨nh duy·ªát khi ho√†n t·∫•t
driver.quit()

print(f"‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {csv_filename}")
